syntax = "proto3";
option optimize_for = LITE_RUNTIME;
// all the matrix are stored in row-major order, 
// plz see https://en.wikipedia.org/wiki/Row-_and_column-major_order for details

// the definition of "Multi-Head Attention", "Scaled Dot-Product Attention" and "Feed-Forward Networks"
// plz see https://arxiv.org/abs/1706.03762 for details

message EncoderLayer {
    // layer norm before "Multi-Head Attention"
    repeated float multihead_norm_scale = 1;
    repeated float multihead_norm_bias = 2;
    
    // "Multi-Head Attention" linearly project weights kernel for query, key, value, 
    // before "Scaled Dot-Product Attention, with shape (hidden_size, hidden_size*3)
    // is built by numpy.concatenate((query_kernel, key_kernel, value_kernel), axis=1)
    // perform numpy.dot(input, multihead_project_kernel_qkv) will get the [query, key, value] of
    // "Scaled Dot-Product Attention"
    repeated float multihead_project_kernel_qkv = 3;
    repeated float multihead_project_bias_qkv = 4;
    // "Multi-Head Attention" linearly project weights kernel for output
    // after "Scaled Dot-Product Attention", with shape (hidden_size, hidden_size)
    repeated float multihead_project_kernel_output = 5;
    repeated float multihead_project_bias_output = 6;

    
    // layer norm after "Feed-Forward Networks"
    repeated float ffn_norm_scale = 7;
    repeated float ffn_norm_bias = 8;
    
    // "Feed-Forward Networks"
    repeated float ffn_first_kernel = 9;
    repeated float ffn_first_bias = 10;
    repeated float ffn_second_kernel = 11;
    repeated float ffn_second_bias = 12;
}

message DecoderLayer {
    // decoder-self-attention
    repeated float self_norm_scale = 1;
    repeated float self_norm_bias = 2;
    repeated float self_project_kernel_qkv = 3;
    repeated float self_project_bias_qkv = 4;
    repeated float self_project_kernel_output = 5;
    repeated float self_project_bias_output = 6;

    // decoder-encode-attention
    repeated float encdec_norm_scale = 7;
    repeated float encdec_norm_bias = 8;
    repeated float encdec_project_kernel_q = 9;
    repeated float encdec_project_bias_q = 10;
    repeated float encdec_project_kernel_output = 11;
    repeated float encdec_project_bias_output = 12;

    // "Feed-Forward Networks"
    repeated float ffn_norm_scale = 13;
    repeated float ffn_norm_bias = 14;
    repeated float ffn_first_kernel = 15;
    repeated float ffn_first_bias = 16;
    repeated float ffn_second_kernel = 17;
    repeated float ffn_second_bias = 18;
}

message EmbeddingLayer {
    // token embedding table
    // for encoder, it is in [src_vocab_size, hidden_size]
    // for decoder, it is in [hidden_size, trg_vocab_size]
    // notice, it shoule have been multiply by sqrt(hidden_size)
    // so, look it up directly will get the input token embedding, there is no need
    // to multiply by sqrt(hidden_size) during inference.
    repeated float token_embedding = 1;
    repeated float position_embedding = 2;
    // the last layer_norm of encoder or decoder
    repeated float norm_scale = 3; 
    repeated float norm_bias = 4; 
    
    // only for trg, not in src
    repeated float encode_output_project_kernel_kv = 5;
    repeated float encode_output_project_bias_kv = 6;
    repeated float shared_bias = 7;
}

message ModelConf {
    int32 head_num = 1;
    int32 beam_size = 2;
    int32 extra_decode_length = 3;
    float length_penalty = 4;
    int32 src_padding_id = 5;
    int32 trg_start_id = 6;
}

message Transformer {
    EmbeddingLayer src_embedding = 1;
    repeated EncoderLayer encoder_stack = 2;
    EmbeddingLayer trg_embedding = 3;
    repeated DecoderLayer decoder_stack = 4;
    ModelConf model_conf = 5;
}
